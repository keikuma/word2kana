{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unidic-cwj-2.3.0.zip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "def download_data(url, force_download=True): \n",
    "    fname = url.split(\"/\")[-1]\n",
    "    if force_download or not os.path.exists(fname):\n",
    "        urllib.request.urlretrieve(url, fname)\n",
    "    return fname\n",
    "\n",
    "url_unidic = \"https://unidic.ninjal.ac.jp/unidic_archive/cwj/2.3.0/unidic-cwj-2.3.0.zip\"\n",
    "unidic = download_data(url_unidic, force_download=False) \n",
    "print(unidic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "master.zip\n"
     ]
    }
   ],
   "source": [
    "url_neologdic = \"https://github.com/neologd/mecab-ipadic-neologd/archive/master.zip\"\n",
    "neologdic = download_data(url_neologdic, force_download=False) \n",
    "print(neologdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unidic-cwj-2.3.0/AUTHORS', 'unidic-cwj-2.3.0/BSD', 'unidic-cwj-2.3.0/ChaMame for Windows/', 'unidic-cwj-2.3.0/ChaMame for Windows/ChaMame Install guide.pdf', 'unidic-cwj-2.3.0/ChaMame for Windows/ChaMameSetup.msi', 'unidic-cwj-2.3.0/char.bin', 'unidic-cwj-2.3.0/char.def', 'unidic-cwj-2.3.0/COPYING', 'unidic-cwj-2.3.0/dicrc', 'unidic-cwj-2.3.0/feature.def', 'unidic-cwj-2.3.0/GPL', 'unidic-cwj-2.3.0/left-id.def', 'unidic-cwj-2.3.0/lex.csv', 'unidic-cwj-2.3.0/LGPL', 'unidic-cwj-2.3.0/matrix.bin', 'unidic-cwj-2.3.0/matrix.def', 'unidic-cwj-2.3.0/model.bin', 'unidic-cwj-2.3.0/model.def', 'unidic-cwj-2.3.0/rewrite.def', 'unidic-cwj-2.3.0/right-id.def', 'unidic-cwj-2.3.0/sys.dic', 'unidic-cwj-2.3.0/unk.def', 'unidic-cwj-2.3.0/unk.dic']\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(unidic) as unidic_zip:\n",
    "    print(unidic_zip.namelist())\n",
    "    unidic_zip.extract('unidic-cwj-2.3.0/lex.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32103\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "unidict = {}\n",
    "pat = re.compile('^([ァ-ヶー]+)-([\\s!-~]+)$')\n",
    "with open('unidic-cwj-2.3.0/lex.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        m = pat.match(row[11])\n",
    "        if m:\n",
    "            unidict[m.group(2)] = m.group(1)\n",
    "print(len(unidict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(neologdic) as neolog_zip:\n",
    "    mecab_user_dict_seed = [n for n in neolog_zip.namelist() if 'mecab-user-dict-seed' in n][0]\n",
    "    neolog_zip.extract(mecab_user_dict_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216588\n"
     ]
    }
   ],
   "source": [
    "import lzma\n",
    "\n",
    "neodict = {}\n",
    "pat = re.compile('^[\\s!-~]+$')\n",
    "with lzma.open(mecab_user_dict_seed, mode='rt') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        if pat.match(row[0]):\n",
    "            neodict[row[0]] = row[11]\n",
    "print(len(neodict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ヘイセイジャンプ\n"
     ]
    }
   ],
   "source": [
    "print(neodict['hey say jump'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245438\n"
     ]
    }
   ],
   "source": [
    "mergedict = dict(unidict)\n",
    "mergedict.update(neodict)\n",
    "print(len(mergedict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "kanas = []\n",
    "\n",
    "for k, v in mergedict.items():\n",
    "    words.append(k)\n",
    "    kanas.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abject アブジェクト\n"
     ]
    }
   ],
   "source": [
    "idx = 1234\n",
    "print(words[idx], kanas[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'セ', 'B', 'f', 'ネ', ']', 'l', '(', 'v', 'Q', 'L', 'ズ', 'ュ', 'ユ', 'ェ', 'ロ', '?', 'w', 'マ', '%', ')', '@', 'ォ', 'ペ', 'ワ', 'ア', '`', 'ギ', 'グ', '\\\\', 'ゥ', 'V', 'K', 'k', 'n', 'b', 'キ', 'O', 'Z', 'P', 'ゲ', 'ャ', 'ビ', 'T', 'N', 'ホ', 'ヌ', 'シ', 'ポ', '3', 'ィ', 'ボ', '*', 'チ', ';', 'ョ', '_', '&', 'ツ', ':', 'ゼ', 'ガ', '/', '9', 'S', 'ヴ', '5', 'ヨ', '#', '}', 'ナ', 'ル', \"'\", 'ヲ', '-', '|', '7', 'z', 'U', ' ', '[', 'デ', 'M', 'ニ', 'u', 'ヂ', 'パ', 'Y', 'テ', 'ジ', ',', 'メ', '㋘', 'c', 'ノ', '4', 'ブ', 'a', 'タ', 'オ', 'F', 't', 'ァ', 'D', 'ソ', 'ケ', '+', 'フ', '1', 'ヘ', '\\u3000', 'ト', '.', 'ク', 'x', '~', 'd', '<', 'ミ', 'ス', 'ピ', 'E', 'リ', 'J', '{', 'エ', 'ヒ', 'ヱ', 'ッ', 'ヅ', '>', 'q', 'p', 'm', 'W', 'イ', 'ヮ', 'g', 'ゾ', '!', 'i', 'C', '^', 'ド', 'ウ', 'レ', 'コ', '0', 'H', 'e', 'o', 'y', 'X', 'サ', '=', 'ヤ', 'ベ', 'モ', 'j', 'r', 'A', 'カ', '$', 'R', 'ダ', 'ー', 'プ', 'ム', 'ザ', 'h', 'ゴ', 'ラ', 'G', 'I', '8', '6', 'ヰ', 'バ', 'ハ', 's', '2', 'ン'}\n",
      "181\n"
     ]
    }
   ],
   "source": [
    "all_chars = set()\n",
    "for word, kana in zip(words, kanas):\n",
    "    for c in word:\n",
    "        all_chars.add(c)\n",
    "    for c in kana:\n",
    "        all_chars.add(c)\n",
    "print(all_chars)\n",
    "print(len(all_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_set = sorted(list(all_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\u3000', 'ァ', 'ア', 'ィ', 'イ', 'ゥ', 'ウ', 'ェ', 'エ', 'ォ', 'オ', 'カ', 'ガ', 'キ', 'ギ', 'ク', 'グ', 'ケ', 'ゲ', 'コ', 'ゴ', 'サ', 'ザ', 'シ', 'ジ', 'ス', 'ズ', 'セ', 'ゼ', 'ソ', 'ゾ', 'タ', 'ダ', 'チ', 'ヂ', 'ッ', 'ツ', 'ヅ', 'テ', 'デ', 'ト', 'ド', 'ナ', 'ニ', 'ヌ', 'ネ', 'ノ', 'ハ', 'バ', 'パ', 'ヒ', 'ビ', 'ピ', 'フ', 'ブ', 'プ', 'ヘ', 'ベ', 'ペ', 'ホ', 'ボ', 'ポ', 'マ', 'ミ', 'ム', 'メ', 'モ', 'ャ', 'ヤ', 'ュ', 'ユ', 'ョ', 'ヨ', 'ラ', 'リ', 'ル', 'レ', 'ロ', 'ヮ', 'ワ', 'ヰ', 'ヱ', 'ヲ', 'ン', 'ヴ', 'ー', '㋘']\n"
     ]
    }
   ],
   "source": [
    "print(symbol_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_symbol_index(word):\n",
    "    return [symbol_set.index(char) for char in word]\n",
    "\n",
    "def symbol_index_to_word(indices):\n",
    "    return [symbol_set[idx] for idx in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64, 65, 73, 68, 66, 83] ['a', 'b', 'j', 'e', 'c', 't']\n",
      "[96, 148, 118, 101, 109, 134] ['ア', 'ブ', 'ジ', 'ェ', 'ク', 'ト']\n"
     ]
    }
   ],
   "source": [
    "idx = 1234\n",
    "indices_word = word_to_symbol_index(words[idx])\n",
    "print(indices_word, symbol_index_to_word(indices_word))\n",
    "indices_kana = word_to_symbol_index(kanas[idx])\n",
    "print(indices_kana, symbol_index_to_word(indices_kana))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dataX = []\n",
    "for word in words:\n",
    "    dataX.append(np.array(word_to_symbol_index(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([84, 77, 67, 68, 81, 70, 81, 78, 84, 77, 67]),\n",
       " ['u', 'n', 'd', 'e', 'r', 'g', 'r', 'o', 'u', 'n', 'd'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 2048\n",
    "dataX[idx], symbol_index_to_word(dataX[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataY =[]\n",
    "for k in kanas:\n",
    "    dataY.append(np.array(word_to_symbol_index(k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 96, 177, 126, 179, 110, 167, 100, 177, 135]),\n",
       " ['ア', 'ン', 'ダ', 'ー', 'グ', 'ラ', 'ウ', 'ン', 'ド'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataY[idx], symbol_index_to_word(dataY[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC:  ['u', 'n', 'd', 'e', 'r', 'g', 'r', 'o', 'u', 'n', 'd']\n",
      "TRG:  ['ア', 'ン', 'ダ', 'ー', 'グ', 'ラ', 'ウ', 'ン', 'ド']\n",
      "SRC:  [84 77 67 68 81 70 81 78 84 77 67]\n",
      "TRG:  [ 96 177 126 179 110 167 100 177 135]\n"
     ]
    }
   ],
   "source": [
    "print(\"SRC: \", symbol_index_to_word(dataX[idx]))\n",
    "print(\"TRG: \", symbol_index_to_word(dataY[idx])) \n",
    "print(\"SRC: \", dataX[idx])\n",
    "print(\"TRG: \", dataY[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_together(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "dataX, dataY = np.array(dataX), np.array(dataY)\n",
    "dataX, dataY = shuffle_together(dataX, dataY)\n",
    "\n",
    "N = int(len(dataX) * 0.9) # 90%\n",
    "\n",
    "### First 4 indices are saved for special characters ###\n",
    "\n",
    "trainX = dataX[:N] + 4\n",
    "trainY = dataY[:N] + 4\n",
    "\n",
    "valX = dataX[N:] + 4\n",
    "valY = dataY[N:] + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = {c:i + 4 for i,c in enumerate(symbol_set)}\n",
    "vocab_dict\n",
    "PAD_SYMBOL = \"<pad>\" #0\n",
    "UNK_SYMBOL = \"<unk>\" #1\n",
    "BOS_SYMBOL = \"<s>\" #2\n",
    "EOS_SYMBOL = \"</s>\" #3\n",
    "\n",
    "VOCAB_SYMBOLS = [PAD_SYMBOL, UNK_SYMBOL, BOS_SYMBOL, EOS_SYMBOL]\n",
    "vocab_dict[PAD_SYMBOL] = 0\n",
    "vocab_dict[UNK_SYMBOL] = 1\n",
    "vocab_dict[BOS_SYMBOL] = 2\n",
    "vocab_dict[EOS_SYMBOL] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('vocab.src.json', 'w') as fp:\n",
    "    json.dump(vocab_dict, fp, indent=4, ensure_ascii=False)\n",
    "        \n",
    "with open('vocab.trg.json', 'w') as fp:\n",
    "    json.dump(vocab_dict, fp, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing \n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "\n",
    "sys.path.append('./SageMaker_seq2seq_WordPronunciation')\n",
    "from typing import List\n",
    "from record_pb2 import Record ### record_pb2.py\n",
    "from create_vocab_proto import write_worker, write_recordio, list_to_record_bytes, read_worker\n",
    "import struct\n",
    "import io\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(np_dataX, np_dataY, file_type, output_file):\n",
    "    num_read_workers = max(multiprocessing.cpu_count() - 1, 1) \n",
    "    logger.info('Spawning %s encoding worker(s) for encoding %s datasets!', str(num_read_workers), file_type) \n",
    "    \n",
    "    q_in = [multiprocessing.Queue() for i in range(num_read_workers)] \n",
    " \n",
    "    q_out = multiprocessing.Queue() \n",
    "\n",
    "    read_process = [multiprocessing.Process(target=read_worker,\n",
    "                    args=(q_in[i], q_out)) for i in range(num_read_workers)] \n",
    "   \n",
    "    for p in read_process: \n",
    "        p.start()\n",
    "\n",
    "    write_process = multiprocessing.Process(target=write_worker, args=(q_out, output_file)) \n",
    "    write_process.start() \n",
    "    \n",
    "    lines_ignored = 0 # No ignored lines in this example. \n",
    "    lines_processed = 0\n",
    "    \n",
    "    for i, int_source  in enumerate(np_dataX):\n",
    "        int_source = int_source.tolist()\n",
    "        int_target = np_dataY[i].tolist()\n",
    "        item = (int_source, int_target) ### <class 'list'>, <class 'list'>\n",
    "\n",
    "        if random.random() < 0.0001:\n",
    "            ### Print some SRC-TRG pairs. \n",
    "            print('===   ===   ===   ===   ===')\n",
    "            print('SRC:', int_source)\n",
    "            print(len(int_source), type(int_source), type(int_source[0])) # num <class 'list'> <class 'int'>\n",
    "            print('---   ---   ---   ---   ---')\n",
    "            print('TRG:', int_target)\n",
    "            print(len(int_target), type(int_target), type(int_target[0])) # num <class 'list'> <class 'int'>\n",
    "\n",
    "        q_in[lines_processed % len(q_in)].put(item) \n",
    "\n",
    "        lines_processed += 1 \n",
    "    \n",
    "    logger.info(\"\"\"Processed %s lines for encoding to protobuf. %s lines were ignored as they didn't have\n",
    "                any content in either the source or the target file!\"\"\", lines_processed, lines_ignored)\n",
    "    \n",
    "    logger.info('Completed writing the encoding queue!')\n",
    "\n",
    "    for q in q_in: \n",
    "        q.put(None) \n",
    "    for p in read_process: \n",
    "        p.join()\n",
    "    logger.info('Encoding finished! Writing records to \"%s\"', output_file)\n",
    "    q_out.put(None) \n",
    "    write_process.join() \n",
    "    logger.info('Processed input and saved to \"%s\"', output_file)\n",
    "    print('+++---+++---+++---+++---+++')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Spawning 1 encoding worker(s) for encoding train datasets!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===   ===   ===   ===   ===\n",
      "SRC: [20, 27, 22, 80, 68]\n",
      "5 <class 'list'> <class 'int'>\n",
      "---   ---   ---   ---   ---\n",
      "TRG: [148, 165, 113, 145, 131, 122, 167, 104, 119, 181, 161, 172, 100, 181, 156, 100]\n",
      "16 <class 'list'> <class 'int'>\n",
      "===   ===   ===   ===   ===\n",
      "SRC: [55, 75, 72, 4, 43, 82, 78, 88, 87, 82, 4, 37, 68, 81, 78, 4, 47, 87, 71, 17]\n",
      "20 <class 'list'> <class 'int'>\n",
      "---   ---   ---   ---   ---\n",
      "TRG: [157, 113, 138, 112, 181, 117, 104]\n",
      "7 <class 'list'> <class 'int'>\n",
      "===   ===   ===   ===   ===\n",
      "SRC: [92, 88, 16, 81, 68, 4, 78, 76, 80]\n",
      "9 <class 'list'> <class 'int'>\n",
      "---   ---   ---   ---   ---\n",
      "TRG: [170, 140, 111, 162]\n",
      "4 <class 'list'> <class 'int'>\n",
      "===   ===   ===   ===   ===\n",
      "SRC: [54, 75, 76, 81, 78, 82, 4, 58, 76, 85, 72, 4, 38, 82, 80, 83, 68, 81, 92]\n",
      "19 <class 'list'> <class 'int'>\n",
      "---   ---   ---   ---   ---\n",
      "TRG: [121, 181, 117, 104, 117, 104, 125, 181, 117, 104, 112, 169, 104]\n",
      "13 <class 'list'> <class 'int'>\n",
      "===   ===   ===   ===   ===\n",
      "SRC: [87, 68, 79, 76, 86, 80, 68, 81]\n",
      "8 <class 'list'> <class 'int'>\n",
      "---   ---   ---   ---   ---\n",
      "TRG: [129, 172, 124, 160, 181]\n",
      "5 <class 'list'> <class 'int'>\n",
      "===   ===   ===   ===   ===\n",
      "SRC: [81, 86, 74]\n",
      "3 <class 'list'> <class 'int'>\n",
      "---   ---   ---   ---   ---\n",
      "TRG: [106, 142, 106, 123, 122, 183]\n",
      "6 <class 'list'> <class 'int'>\n",
      "===   ===   ===   ===   ===\n",
      "SRC: [79, 92, 81]\n",
      "3 <class 'list'> <class 'int'>\n",
      "---   ---   ---   ---   ---\n",
      "TRG: [172, 181]\n",
      "2 <class 'list'> <class 'int'>\n",
      "===   ===   ===   ===   ===\n",
      "SRC: [51, 72, 85, 86]\n",
      "4 <class 'list'> <class 'int'>\n",
      "---   ---   ---   ---   ---\n",
      "TRG: [147, 183, 123]\n",
      "3 <class 'list'> <class 'int'>\n",
      "===   ===   ===   ===   ===\n",
      "SRC: [81, 68, 78, 72, 71, 4, 71, 72, 86, 76, 85, 72]\n",
      "12 <class 'list'> <class 'int'>\n",
      "---   ---   ---   ---   ---\n",
      "TRG: [143, 102, 111, 133, 139, 137, 120, 102, 100]\n",
      "9 <class 'list'> <class 'int'>\n",
      "===   ===   ===   ===   ===\n",
      "SRC: [77, 72, 68, 81, 81, 72, 4, 79, 72, 72]\n",
      "10 <class 'list'> <class 'int'>\n",
      "---   ---   ---   ---   ---\n",
      "TRG: [122, 183, 181, 172, 183]\n",
      "5 <class 'list'> <class 'int'>\n",
      "===   ===   ===   ===   ===\n",
      "SRC: [46, 68, 81, 87, 68, 78, 82, 88, 93, 72, 81, 82, 86]\n",
      "13 <class 'list'> <class 'int'>\n",
      "---   ---   ---   ---   ---\n",
      "TRG: [109, 181, 129, 113, 126, 144, 123]\n",
      "7 <class 'list'> <class 'int'>\n",
      "===   ===   ===   ===   ===\n",
      "SRC: [80, 72, 70, 70, 68, 81, 82]\n",
      "7 <class 'list'> <class 'int'>\n",
      "---   ---   ---   ---   ---\n",
      "TRG: [163, 109, 144]\n",
      "3 <class 'list'> <class 'int'>\n",
      "===   ===   ===   ===   ===\n",
      "SRC: [53, 72, 68, 86, 82, 88, 79]\n",
      "7 <class 'list'> <class 'int'>\n",
      "---   ---   ---   ---   ---\n",
      "TRG: [172, 100, 127, 104, 173]\n",
      "5 <class 'list'> <class 'int'>\n",
      "===   ===   ===   ===   ===\n",
      "SRC: [40, 77, 76, 4, 82, 92, 72, 90, 82, 79, 72]\n",
      "11 <class 'list'> <class 'int'>\n",
      "---   ---   ---   ---   ---\n",
      "TRG: [106, 122, 108, 102, 105, 104, 107, 174]\n",
      "8 <class 'list'> <class 'int'>\n",
      "===   ===   ===   ===   ===\n",
      "SRC: [68, 81, 71, 79, 76, 81, 88, 91]\n",
      "8 <class 'list'> <class 'int'>\n",
      "---   ---   ---   ---   ---\n",
      "TRG: [100, 181, 139, 172, 140, 133, 113, 123]\n",
      "8 <class 'list'> <class 'int'>\n",
      "===   ===   ===   ===   ===\n",
      "SRC: [90, 24, 22, 86]\n",
      "4 <class 'list'> <class 'int'>\n",
      "---   ---   ---   ---   ---\n",
      "TRG: [130, 152, 172, 167, 183, 118, 183, 119, 181, 106, 123]\n",
      "11 <class 'list'> <class 'int'>\n",
      "===   ===   ===   ===   ===\n",
      "SRC: [76, 81, 4, 87, 75, 72, 4, 69, 79, 88, 72, 4, 86, 75, 76, 85, 87]\n",
      "17 <class 'list'> <class 'int'>\n",
      "---   ---   ---   ---   ---\n",
      "TRG: [102, 181, 120, 152, 173, 183, 121, 165, 134]\n",
      "9 <class 'list'> <class 'int'>\n",
      "===   ===   ===   ===   ===\n",
      "SRC: [24, 23, 27, 46]\n",
      "4 <class 'list'> <class 'int'>\n",
      "---   ---   ---   ---   ---\n",
      "TRG: [118, 148, 165, 113, 170, 181, 122, 167, 104, 145, 131, 115, 173, 149, 181]\n",
      "15 <class 'list'> <class 'int'>\n",
      "===   ===   ===   ===   ===\n",
      "SRC: [85, 82, 71, 72, 82, 4, 70, 68, 85, 69, 88, 85, 72, 87, 87, 82, 85]\n",
      "17 <class 'list'> <class 'int'>\n",
      "---   ---   ---   ---   ---\n",
      "TRG: [175, 137, 108, 111, 165, 152, 174, 129, 183]\n",
      "9 <class 'list'> <class 'int'>\n",
      "===   ===   ===   ===   ===\n",
      "SRC: [36, 85, 71, 72, 81]\n",
      "5 <class 'list'> <class 'int'>\n",
      "---   ---   ---   ---   ---\n",
      "TRG: [100, 183, 137, 181]\n",
      "4 <class 'list'> <class 'int'>\n",
      "===   ===   ===   ===   ===\n",
      "SRC: [79, 76, 82, 81, 4, 82, 73, 73, 76, 70, 72, 4, 83, 85, 82, 71, 88, 70, 87, 86]\n",
      "20 <class 'list'> <class 'int'>\n",
      "---   ---   ---   ---   ---\n",
      "TRG: [171, 102, 108, 181, 122, 162, 111]\n",
      "7 <class 'list'> <class 'int'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processed 220894 lines for encoding to protobuf. 0 lines were ignored as they didn't have\n",
      "                any content in either the source or the target file!\n",
      "INFO:__main__:Completed writing the encoding queue!\n",
      "INFO:__main__:Encoding finished! Writing records to \"train.rec\"\n",
      "INFO:__main__:Processed input and saved to \"train.rec\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++---+++---+++---+++---+++\n"
     ]
    }
   ],
   "source": [
    "file_type = 'train'\n",
    "output_file = \"train.rec\"\n",
    "write_to_file(trainX, trainY, file_type, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Spawning 1 encoding worker(s) for encoding validation datasets!\n",
      "INFO:__main__:Processed 24544 lines for encoding to protobuf. 0 lines were ignored as they didn't have\n",
      "                any content in either the source or the target file!\n",
      "INFO:__main__:Completed writing the encoding queue!\n",
      "INFO:__main__:Encoding finished! Writing records to \"val.rec\"\n",
      "INFO:__main__:Processed input and saved to \"val.rec\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++---+++---+++---+++---+++\n"
     ]
    }
   ],
   "source": [
    "file_type = 'validation'\n",
    "output_file = \"val.rec\"\n",
    "write_to_file(valX, valY, file_type, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'sagemaker-word2kana'\n",
    "prefix = 'seq2seq/word2kana'  \n",
    "\n",
    "import boto3\n",
    "\n",
    "def upload_to_s3(bucket, prefix, channel, file):\n",
    "    s3 = boto3.resource('s3')\n",
    "    data = open(file, \"rb\")\n",
    "    key = prefix + \"/\" + channel + '/' + file\n",
    "    s3.Bucket(bucket).put_object(Key=key, Body=data)\n",
    "\n",
    "upload_to_s3(bucket, prefix, 'train', 'train.rec') \n",
    "upload_to_s3(bucket, prefix, 'validation', 'val.rec') \n",
    "upload_to_s3(bucket, prefix, 'vocab', 'vocab.src.json') \n",
    "upload_to_s3(bucket, prefix, 'vocab', 'vocab.trg.json') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_name = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'us-east-2'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SageMaker Seq2Seq container: 825641698319.dkr.ecr.us-east-2.amazonaws.com/seq2seq:latest (us-east-2)\n"
     ]
    }
   ],
   "source": [
    "containers = {'us-west-2': '433757028032.dkr.ecr.us-west-2.amazonaws.com/seq2seq:latest',\n",
    "              'us-east-1': '811284229777.dkr.ecr.us-east-1.amazonaws.com/seq2seq:latest',\n",
    "              'us-east-2': '825641698319.dkr.ecr.us-east-2.amazonaws.com/seq2seq:latest',\n",
    "              'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com/seq2seq:latest'}\n",
    "container = containers[region_name]\n",
    "print('Using SageMaker Seq2Seq container: {} ({})'.format(container, region_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job seq2seq-wrd-phn-p2-xlarge-2018-12-23-08-00\n",
      "InProgress\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "source_sequence_length = max([len(w) for w in words])\n",
    "target_sequence_length = max([len(k) for k in kanas])\n",
    "\n",
    "job_name = 'seq2seq-wrd-phn-p2-xlarge-' + strftime(\"%Y-%m-%d-%H-%M\", gmtime())\n",
    "print(\"Training job\", job_name)\n",
    "\n",
    "create_training_params = \\\n",
    "{\n",
    "    \"AlgorithmSpecification\": {\n",
    "        \"TrainingImage\": container,\n",
    "        \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"OutputDataConfig\": {\n",
    "        \"S3OutputPath\": \"s3://{}/{}/\".format(bucket, prefix)\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "        # Seq2Seq does not support multiple machines. Currently, it only supports single machine, multiple GPUs\n",
    "        \"InstanceCount\": 1,\n",
    "        \"InstanceType\": \"ml.p2.xlarge\", # We suggest one of [\"ml.p2.16xlarge\", \"ml.p2.8xlarge\", \"ml.p2.xlarge\"]\n",
    "        \"VolumeSizeInGB\": 50\n",
    "    },\n",
    "    \"TrainingJobName\": job_name,\n",
    "    \"HyperParameters\": {\n",
    "        # Please refer to the documentation for complete list of parameters\n",
    "        \"max_seq_len_source\": str(source_sequence_length),\n",
    "        \"max_seq_len_target\": str(target_sequence_length),\n",
    "        \"optimized_metric\": \"bleu\", \n",
    "        \"batch_size\": \"128\", # Please use a larger batch size (256 or 512) if using ml.p2.8xlarge or ml.p2.16xlarge\n",
    "        \"checkpoint_frequency_num_batches\": \"1000\",\n",
    "        \"rnn_num_hidden\": \"512\",\n",
    "        \"num_layers_encoder\": \"1\",\n",
    "        \"num_layers_decoder\": \"1\",\n",
    "        \"num_embed_source\": \"512\",\n",
    "        \"num_embed_target\": \"512\",\n",
    "        \"checkpoint_threshold\": \"3\",\n",
    "        #\"max_num_batches\": \"2100\"\n",
    "        # Training will stop after 2100 iterations/batches.\n",
    "        # This is just for demo purposes. Remove the above parameter if you want a better model.\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "        \"MaxRuntimeInSeconds\": 48 * 3600\n",
    "    },\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": \"s3://{}/{}/train/\".format(bucket, prefix),\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"vocab\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": \"s3://{}/{}/vocab/\".format(bucket, prefix),\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"validation\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": \"s3://{}/{}/validation/\".format(bucket, prefix),\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "sagemaker_client = boto3.Session().client(service_name='sagemaker')\n",
    "sagemaker_client.create_training_job(**create_training_params)\n",
    "\n",
    "status = sagemaker_client.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InProgress\n"
     ]
    }
   ],
   "source": [
    "### Please keep on checking the status until this says \"Completed\". ###\n",
    "\n",
    "status = sagemaker_client.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "print(status)\n",
    "# if the job failed, determine why\n",
    "if status == 'Failed':\n",
    "    message = sagemaker_client.describe_training_job(TrainingJobName=job_name)['FailureReason']\n",
    "    print('Training failed with the following error: {}'.format(message))\n",
    "    raise Exception('Training job failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
